{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import normalize, softmax, cosine_similarity\n",
    "from torchvision.transforms.functional import resize, rgb_to_grayscale\n",
    "from torchvision.utils import save_image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.dataset import VoiceToFaceDataset\n",
    "from src.model.eigenface import Eigenface\n",
    "from src.model.mlp import MLP\n",
    "from src.model.voice_embedder import forge_voice_embedder_with_parameters, DEFAULT_OUTPUT_FEATURE_NUM, VoiceEmbedNet\n",
    "from src.model.generator import forge_generator_with_parameters, Generator\n",
    "from src.config import TrainingConfig\n",
    "from src.utils import get_tensor_device\n",
    "from src.model.resnet import resnet50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset from datasets/metadata-test.csv.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572112def214493db4bc32464927536d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset...:   0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = VoiceToFaceDataset(\n",
    "    Path('datasets/voices/'),\n",
    "    Path('datasets/images/'),\n",
    "    Path('datasets/metadata-test.csv'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TARGET_PATH = Path('checkpoints/training/2022-12-9-2-15-21')\n",
    "# TARGET_EPOCH = 25\n",
    "TARGET_PATH = Path('checkpoints/training/2022-12-9-15-31-52')\n",
    "TARGET_EPOCH = 12\n",
    "\n",
    "training_config = TrainingConfig.from_json(TARGET_PATH / 'config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Eigenface converter with eigenface_components = 5000.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voice_embedder = forge_voice_embedder_with_parameters(\n",
    "    Path('checkpoints/voice_embedding.pth')\n",
    ").to(device).eval()\n",
    "generator = forge_generator_with_parameters(\n",
    "    Path('checkpoints/generator.pth')\n",
    ").to(device).eval()\n",
    "eigenface_converter = Eigenface(Path('checkpoints/input-15k-pc-5k.npy')).to(device)\n",
    "mlp = MLP(\n",
    "    DEFAULT_OUTPUT_FEATURE_NUM,\n",
    "    eigenface_converter.eigenface_components,\n",
    "    training_config.mlp_hidden_size,\n",
    "    training_config.mlp_hidder_layer_num,\n",
    "    training_config.mlp_dropout_probability,\n",
    ").to(device).eval()\n",
    "mlp.load_state_dict(\n",
    "    torch.load(TARGET_PATH / f'mlp-{TARGET_EPOCH}.pth', map_location=device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet = resnet50(num_classes=8631).to(device).eval()\n",
    "with open('checkpoints/resnet50_scratch_weight.pkl', 'rb') as f:\n",
    "    ckpt = pickle.load(f)\n",
    "    ckpt = {k: torch.from_numpy(v).to(device) for k, v in ckpt.items()}\n",
    "resnet.load_state_dict(ckpt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def reconstruct_ours(feature: Tensor, v: VoiceEmbedNet, m: MLP, e: Eigenface):\n",
    "    if feature.dim() == 2:\n",
    "        feature = feature.unsqueeze(0)\n",
    "    voice_embedding = v(feature)\n",
    "    voice_embedding = rearrange(voice_embedding, 'N C 1 1 -> N C')\n",
    "    eigenface = m(voice_embedding)\n",
    "    reconstructed_face = eigenface_converter.eigenface_to_face(eigenface).reshape(128, 128)\n",
    "    reconstructed_face = resize(reconstructed_face.unsqueeze(0), [64, 64])\n",
    "    return reconstructed_face\n",
    "\n",
    "@torch.inference_mode()\n",
    "def reconstruct_yans(feature: Tensor, v: VoiceEmbedNet, g: Generator):\n",
    "    if feature.dim() == 2:\n",
    "        feature = feature.unsqueeze(0)\n",
    "    voice_embedding = v(feature)\n",
    "    reconstructed_face: Tensor = generator(voice_embedding)\n",
    "    reconstructed_face = rgb_to_grayscale(reconstructed_face)\n",
    "    reconstructed_face = rearrange(reconstructed_face, '1 1 W H -> 1 W H')\n",
    "    return reconstructed_face"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375e9c9e40b24bad824f748c8569047a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ours_cosine_similarities = defaultdict(list)\n",
    "yans_cosine_similarities = defaultdict(list)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i in tqdm(range(len(test_dataset))):\n",
    "        if i % 2000 == 0:\n",
    "            print(f'Current {i = }.')\n",
    "        voice_feature, voice_id, ground_truth_images, name = test_dataset[i]\n",
    "        voice_feature = voice_feature.to(device).unsqueeze(0)\n",
    "        ground_truth_images = ground_truth_images.to(device)\n",
    "        ground_truth_images = rearrange(ground_truth_images, 'N (H W) -> N 1 H W', W=128)\n",
    "        ground_truth_images = resize(ground_truth_images, [64, 64])\n",
    "        ground_truth_images = torch.cat([ground_truth_images for _ in range(3)], dim=1)\n",
    "        \n",
    "        our_reconstruction = reconstruct_ours(voice_feature, voice_embedder, mlp, eigenface_converter)\n",
    "        our_reconstruction = rearrange(our_reconstruction, '1 H W -> 1 1 H W')\n",
    "        our_reconstruction = torch.cat([our_reconstruction for _ in range(3)], dim=1)\n",
    "\n",
    "        yan_reconstruction = reconstruct_yans(voice_feature, voice_embedder, generator)\n",
    "        yan_reconstruction = rearrange(yan_reconstruction, '1 H W -> 1 1 H W')\n",
    "        yan_reconstruction = torch.cat([yan_reconstruction for _ in range(3)], dim=1)\n",
    "        \n",
    "        ours_embedding = resnet(our_reconstruction)\n",
    "        yans_embedding = resnet(yan_reconstruction)\n",
    "        ground_truth_embeddings = resnet(ground_truth_images)\n",
    "        \n",
    "        ours_cosine_similarity = cosine_similarity(ours_embedding, ground_truth_embeddings).mean().item()\n",
    "        yans_cosine_similarity = cosine_similarity(yans_embedding, ground_truth_embeddings).mean().item()\n",
    "        ours_cosine_similarities[name].append(ours_cosine_similarity)\n",
    "        yans_cosine_similarities[name].append(yans_cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ours_cosine_similarities_mean = torch.tensor([\n",
    "    torch.tensor(v).mean().item() for v in ours_cosine_similarities.values()\n",
    "]).mean().item()\n",
    "yans_cosine_similarities_mean = torch.tensor([\n",
    "    torch.tensor(v).mean().item() for v in yans_cosine_similarities.values()\n",
    "]).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5907018184661865, 0.41483274102211)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ours_cosine_similarities_mean, yans_cosine_similarities_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4c20c992130420a459803d4289592358ac92ed4bba13ba69dad185cb3aeef22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
